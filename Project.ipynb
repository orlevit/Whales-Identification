{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------Humpback Whale Identification----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "################################################### Imports ################################################################\n",
    "\n",
    "import PIL\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import shutil\n",
    "import csv\n",
    "import cv2\n",
    "import scipy.misc\n",
    "import time\n",
    "import math\n",
    "import numpy               as np \n",
    "import pandas              as pd\n",
    "import matplotlib.pyplot   as plt\n",
    "import imgaug              as ia\n",
    "\n",
    "from PIL                   import Image\n",
    "from tqdm                  import tqdm\n",
    "from os.path               import isfile\n",
    "from imagehash             import phash\n",
    "from shutil                import copyfile\n",
    "from imgaug                import augmenters as iaa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing   import image\n",
    "from collections           import Counter\n",
    "from keras.models          import load_model\n",
    "from keras.metrics         import top_k_categorical_accuracy\n",
    "from keras.optimizers      import Adam\n",
    "from keras.models          import Model\n",
    "from keras.models          import load_model\n",
    "from keras.callbacks       import ModelCheckpoint\n",
    "from keras.callbacks       import EarlyStopping\n",
    "from keras                 import regularizers\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization, AveragePooling2D\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################  Constants ############################################################################\n",
    "\n",
    "SAMPLE_FILE           = '/home/or/whales/sample_submission.csv' # The sample submission file\n",
    "TEST_FOLDER           = '/home/or/whales/test'                  # Folder of all the test pic\n",
    "TRAINING_FILE         = \"./train.csv\"                           # The test csv file\n",
    "TRAIN_FOLDER          = '/home/or/whales/train'                 # Folder of all the train pic\n",
    "H2PS                  = \"/home/or/whales/h2ps.pickle\"           # All the oic tha has the same hash\n",
    "P2H                   = \"/home/or/whales/p2h.pickle\"            # Pic to hash matching\n",
    "H2P                   = \"/home/or/whales/h2p.pickle\"            # Hash to pic matching\n",
    "P2SIZE                = \"/home/or/whales/p2size.pickle\"         # Size of picures\n",
    "CONV_TRAIN_FILE       = \"/home/or/whales/conv_train.csv\"        # Train.csv after update close pictures\n",
    "CONV_SUB_FILE         = \"/home/or/whales/conv_sub.csv\"          # Submission.csv after update close pictures\n",
    "TRAINING_FILE_3_NO_W  = \"/home/or/whales/train_3channel.csv\"    # Updated train.csv after transform all pic to 3 cahnnels withput 'new_whale'\n",
    "TRAIN_FOLDER_3CHANNEL = '/home/or/whales/train_3channel'        # Updated train.csv pic directory\n",
    "TRAINING_FILE_FINAL_A = \"/home/or/whales/train_aug_final.csv\"   # Updated train.csv after transform with augmentation of pictures\n",
    "TRAINING_FILE_FINAL   = \"/home/or/whales/train_final.csv\"       # Train file without the validation set\n",
    "TRAIN_FOLDER_FINAL    = '/home/or/whales/train_final'           # Directory of updated train.csv after transform with augmentation\n",
    "VALID_FILE_FINAL      = \"/home/or/whales/validation_final.csv\"  # Validation file the final csv file\n",
    "VALID_FOLDER_FINAL    = '/home/or/whales/validation_final'      # Validation directory \n",
    "BB_TRAIN              = '/home/or/whales/BB_train.csv'          # Boundin boxes file for training\n",
    "BB_TEST               = '/home/or/whales/BB_test.csv'           # Boundin boxes file for test\n",
    "SUBMISSION_FILE       = '/home/or/whales/submission.csv'        # Submission file\n",
    "HISTORY_FILE          = '/home/or/whales/history.pkl'           # The history of the model\n",
    "MODEL_TIME            = '/home/or/whales/Model_tictoc.pkl'      # The runung time for the model\n",
    "BATCH_SIZE            = 100                                     # Batch size\n",
    "IMAGE_SIZE            = 224                                     # Image one dimention\n",
    "TARGET_SIZES          = (IMAGE_SIZE,IMAGE_SIZE)                 # Total image dimentions\n",
    "CLASSES               = 5004                                    # Number of classes\n",
    "EPOCHS                = 120                                     # Number of epochs\n",
    "VALIDATION_STEPS      = 100                                     # Number of steps in the validation\n",
    "NEW_WHALE_INDEX       = 0                                       # Index of the nw whale for the test time augmentation\n",
    "NUMBER_OF_PREDICATIONS= 5                                       # Number of predications per whales, for the test time augmentation\n",
    "THRESHOLD             = 0.276                                   # Threshold of certainty for the test time augmentation\n",
    "MODEL_FILE            = \"/home/or/whales/Or_model.h5\"           # Name if the the previous trained model \n",
    "VALIDIDATION_PRECENT  = 0.3                                     # Number of pictures taken to the validation\n",
    "EPOCH_WAIT_IMPROVE    = 5                                       # After this number of epochs, if thre is no improvment in the validation,then stop\n",
    "\n",
    "###############################################  Indications ############################################################################\n",
    "START_FROM_SCRATCH    = True                                   # Indication if to do all the preprocessing all over again(or not: False)\n",
    "DO_TRAINING           = True                                    # Indication if to train the model again or use the existing one(or not: False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------PREPROCESSING--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding similar images and update the records\n",
    "There are very blurred pictures and closely related ones, apperntly there were couple of similar images(almost identical)\n",
    "that had the same hash(Based on the create hash for picture to find copy rights violations - https://www.phash.org/).\n",
    "The idea was taken by the following https://www.kaggle.com/seesee/siamese-pretrained-0-822, and was added the following presumptions and implementations:\n",
    "\n",
    "    * Same pictures with the (very) close hash are identical\n",
    "    * Pictures with the better resolution are classified better, so that one were picked among all pic with certain hash.\n",
    "    * If one pic were similar to another in the sample, then the one in the sample got the Id of the test\n",
    "\n",
    "In practice only 2 of those cases were found:\n",
    "    a. Two close related pictures, once visualized they looked identical,so it was decided to delete one of them.\n",
    "    b. One picture apeear in th test as well as in the train, so it's Id(New whale) copied into the test csv file.\n",
    "    \n",
    "The implementation were divided into two parts:\n",
    "    1. Finding the closely related pictures.\n",
    "    2. Change the Id of similar whales pic to the one with the best resolution, and check train-submission exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2SIZE exists.\n",
      "P2H exists.\n"
     ]
    }
   ],
   "source": [
    "####################################### Finding similar images and update the records:Part 1 ############################################################################\n",
    "\n",
    "if START_FROM_SCRATCH:\n",
    "    df = pd.read_csv(TRAINING_FILE)\n",
    "    tagged = dict([(p, w) for _, p, w in df.to_records()])\n",
    "    submit = [p for _, p, _ in pd.read_csv(SAMPLE_FILE).to_records()]\n",
    "    join = list(tagged.keys()) + submit\n",
    "\n",
    "    def expand_path(p):\n",
    "        if isfile(TRAIN_FOLDER + '/' + p):\n",
    "            return TRAIN_FOLDER + '/' + p\n",
    "        if isfile(TEST_FOLDER + '/' + p):\n",
    "            return TEST_FOLDER + '/' + p\n",
    "        return p\n",
    "\n",
    "    if isfile(P2SIZE):\n",
    "        print(\"P2SIZE exists.\")\n",
    "        with open(P2SIZE, 'rb') as f:\n",
    "            p2size = pickle.load(f)\n",
    "    else:\n",
    "        p2size = {}\n",
    "        for p in tqdm(join):\n",
    "            size = Image.open(expand_path(p)).size\n",
    "            p2size[p] = size\n",
    "        pickling_on = open(P2SIZE,\"wb\")\n",
    "        pickle.dump(p2size, pickling_on)\n",
    "        pickling_on.close()\n",
    "\n",
    "    def match(h1, h2):\n",
    "        for p1 in h2ps[h1]:\n",
    "            for p2 in h2ps[h2]:\n",
    "                i1 = Image.open(expand_path(p1))\n",
    "                i2 = Image.open(expand_path(p2))\n",
    "                if i1.mode != i2.mode or i1.size != i2.size: return False\n",
    "                a1 = np.array(i1)\n",
    "                a1 = a1 - a1.mean()\n",
    "                a1 = a1 / np.sqrt((a1 ** 2).mean())\n",
    "                a2 = np.array(i2)\n",
    "                a2 = a2 - a2.mean()\n",
    "                a2 = a2 / np.sqrt((a2 ** 2).mean())\n",
    "                a = ((a1 - a2) ** 2).mean()\n",
    "                if a > 0.1: return False\n",
    "        return True\n",
    "\n",
    "\n",
    "    if isfile(P2H):\n",
    "        print(\"P2H exists.\")\n",
    "        with open(P2H, 'rb') as f:\n",
    "            p2h = pickle.load(f)\n",
    "    else:\n",
    "        # Compute phash for each image in the training and test set.\n",
    "        p2h = {}\n",
    "        for p in tqdm(join):\n",
    "            img = Image.open(expand_path(p))\n",
    "            h = phash(img)\n",
    "            p2h[p] = h\n",
    "\n",
    "        # Find all images associated with a given phash value.\n",
    "        h2ps = {}\n",
    "        for p, h in p2h.items():\n",
    "            if h not in h2ps: h2ps[h] = []\n",
    "            if p not in h2ps[h]: h2ps[h].append(p)\n",
    "\n",
    "        # Find all distinct phash values\n",
    "        hs = list(h2ps.keys())\n",
    "\n",
    "        # If the images are close enough, associate the two phash values (this is the slow part: n^2 algorithm)\n",
    "        h2h = {}\n",
    "        for i, h1 in enumerate(tqdm(hs)):\n",
    "            for h2 in hs[:i]:\n",
    "                if h1 - h2 <= 6 and match(h1, h2):\n",
    "                    s1 = str(h1)\n",
    "                    s2 = str(h2)\n",
    "                    if s1 < s2: s1, s2 = s2, s1\n",
    "                    h2h[s1] = s2\n",
    "\n",
    "        # Group together images with equivalent phash, and replace by string format of phash (faster and more readable)\n",
    "        for p, h in p2h.items():\n",
    "            h = str(h)\n",
    "            if h in h2h: h = h2h[h]\n",
    "            p2h[p] = h\n",
    "        with open(P2H, 'wb') as f:\n",
    "             pickle.dump(p2h, f)\n",
    "    # For each image id, determine the list of pictures\n",
    "    h2ps = {}\n",
    "    for p, h in p2h.items():\n",
    "        if h not in h2ps: h2ps[h] = []\n",
    "        if p not in h2ps[h]: h2ps[h].append(p)\n",
    "\n",
    "\n",
    "\n",
    "    with open(H2PS, 'wb') as f:\n",
    "        pickle.dump(h2ps, f)\n",
    "\n",
    "    #For each images id, select the prefered image\n",
    "    def prefer(ps):\n",
    "        if len(ps) == 1: return ps[0]\n",
    "        best_p = ps[0]\n",
    "        best_s = p2size[best_p]\n",
    "        for i in range(1, len(ps)):\n",
    "            p = ps[i]\n",
    "            s = p2size[p]\n",
    "            if s[0] * s[1] > best_s[0] * best_s[1]:  # Select the image with highest resolution\n",
    "                best_p = p\n",
    "                best_s = s\n",
    "        return best_p\n",
    "\n",
    "    h2p = {}\n",
    "    for h, ps in h2ps.items():\n",
    "        h2p[h] = prefer(ps)\n",
    "\n",
    "    with open(H2P, 'wb') as f:\n",
    "        pickle.dump(h2p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of changes are:2 out of total:33321\n"
     ]
    }
   ],
   "source": [
    "####################################### Finding similar images and update the records:Part 2 ############################################################################\n",
    "\n",
    "if START_FROM_SCRATCH:\n",
    "    # load previous files\n",
    "    with open(P2H, 'rb') as f:\n",
    "        p2h = pickle.load(f)\n",
    "    with open(H2P, 'rb') as f:\n",
    "        h2p = pickle.load(f)\n",
    "    \n",
    "    # Getting the test and training dtaframes and indication for how many records were alterd \n",
    "    df_t           =  pd.read_csv(TRAINING_FILE).set_index('Image')  \n",
    "    df_s           =  pd.read_csv(SAMPLE_FILE).set_index('Image')     \n",
    "    df_s['Id']     =  ' '\n",
    "    df_t_new_whale =  df_t[(df_t.Id == \"new_whale\")]\n",
    "    handled_files  = 0\n",
    "    Total          = 0\n",
    "\n",
    "    # Loop foe each picture check if it resemble to another one and if so, take it's Id\n",
    "    # Close related training-test pic got their Id from their training pair in the training\n",
    "    for p, h in p2h.items():  \n",
    "        prefered_fig = h2p[h]\n",
    "\n",
    "        #Different cases as their names infer\n",
    "        OriginalInTrain      =  p in df_t.index;\n",
    "        ChangeInTrain        =  prefered_fig in df_t.index;\n",
    "        OriginalInSample     =  p in df_s.index;\n",
    "        ChangeInSample       =  prefered_fig in df_s.index;\n",
    "        OriginalNotNewWhale  =  p not in df_t_new_whale.index;\n",
    "        ChangeNewWhaleTrain  =  ((prefered_fig in df_t_new_whale.index) != 0);\n",
    "\n",
    "        # Check the different cases\n",
    "        if ((OriginalInTrain) and (ChangeInTrain) and (not ChangeNewWhaleTrain)):\n",
    "          if  p != prefered_fig:\n",
    "              handled_files+=1;\n",
    "              df_t.drop(p, inplace = True)\n",
    "\n",
    "        if ((OriginalInSample) and (ChangeInTrain) and (not ChangeNewWhaleTrain)):\n",
    "          df_s.loc[p, 'Id'] = df_t.loc[prefered_fig, 'Id'] \n",
    "          handled_files+=1;\n",
    "\n",
    "        if ((OriginalInTrain) and (ChangeInSample) and (not OriginalNotNewWhale)):\n",
    "          df_s.loc[prefered_fig, 'Id'] = df_t.loc[p, 'Id'] \n",
    "          handled_files+=1;      \n",
    "\n",
    "        Total +=1\n",
    "\n",
    "    df_t.to_csv(CONV_TRAIN_FILE)\n",
    "    df_s.to_csv(CONV_SUB_FILE) \n",
    "    print('The number of changes are:{} out of total:{}'.format(handled_files,Total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting the images\n",
    "There are colored and gray scale images in the DB.\n",
    "The pre-trained CNN model that is used, gets images input with 3 cahnnels, so all the images coverted to it.\n",
    "(gray scales are duplicated 3 times, if they aren't already duplicated).\n",
    "\n",
    "Moreover, the classification of 'New_whale' may be consider as \"we don't know which whale is that\", so it excluded too\n",
    "(It is address later on in the TTA).\n",
    "aditional augmentation were added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### Formatting the images ######################################################\n",
    "\n",
    "if START_FROM_SCRATCH:\n",
    " \n",
    "    # Loading the previous dataframe(without the 'new whale'), delete the directory if already exists\n",
    "    # Change the pictures to 3 channels and save them\n",
    "    df = pd.read_csv(CONV_TRAIN_FILE).set_index('Image')\n",
    "    df =  df[~(df.Id == \"new_whale\")]\n",
    "\n",
    "    if os.path.isdir(TRAIN_FOLDER_3CHANNEL):\n",
    "        shutil.rmtree(TRAIN_FOLDER_3CHANNEL)\n",
    "    os.makedirs(TRAIN_FOLDER_3CHANNEL)\n",
    "\n",
    "    # For each picture transfrom it into 3 channels\n",
    "    for i in df.index:\n",
    "        \n",
    "      image_path = '{}/{}'.format(TRAIN_FOLDER, i)\n",
    "      image_dest = '{}/{}'.format(TRAIN_FOLDER_3CHANNEL, i)\n",
    "        \n",
    "      try:            \n",
    "        image =  cv2.imread(image_path)\n",
    "      except BaseException  as e:\n",
    "        print(\"Error open an image: \"+ str(e))\n",
    "        continue    \n",
    "      cv2.imwrite(image_dest ,image)\n",
    "\n",
    "    df.to_csv(TRAINING_FILE_3_NO_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create augmentations and apply bounding boxes\n",
    "Three augmentations were applied on each picture, for make more diversified DB and for enabling validation data set.\n",
    "The augmentations were:\n",
    "    1. Flip the picture and add Gaussian blur.\n",
    "    2. Translation of the picture in random amount. Left-Up(0-20), Right-Down(0-20) and rescale it.\n",
    "    3. Translation of the picture like before and add flip left-right.\n",
    "\n",
    "Because only the tail is intersting a bounding box were applied for better precision, the bounding boxes\n",
    "were made according to edge of the tail being darker than it surrounding.\n",
    "Gray scale pictures and picture with bad bounding boxes were discarded, so just 16672 were given bounding boxes.\n",
    "All the  other pictures were taken as is and rescale to the model image size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Defining the augmentations #########################################################\n",
    "\n",
    "def Augmentation_GaussianBlur():\n",
    "    seq = iaa.Sequential([\n",
    "        iaa.Fliplr(0.7),                  # Horizontally flip 70% of the images\n",
    "        iaa.GaussianBlur(sigma=(0,1))     # Blur images with a sigma of 0 to1\n",
    "    ])\n",
    "    return(seq)\n",
    "\n",
    "def Augmentation_Scale_Trans(imagenp):\n",
    "    translate_x_beg = random.randint(1,21)\n",
    "    translate_y_beg = random.randint(1,21)\n",
    "    translate_x_end = random.randint(1,21)\n",
    "    translate_y_end = random.randint(1,21)\n",
    "    crop_img = imagenp[translate_x_beg: -translate_x_end, translate_y_beg: -translate_y_end]\n",
    "    img_resize = cv2.resize(crop_img,(IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    return(img_resize)\n",
    "\n",
    "def Augmentation_Scale_Trans_Flip(imagenp):\n",
    "    translate_x_beg = random.randint(1,21)\n",
    "    translate_y_beg = random.randint(1,21)\n",
    "    translate_x_end = random.randint(1,21)\n",
    "    translate_y_end = random.randint(1,21)\n",
    "    crop_img = imagenp[translate_x_beg: -translate_x_end, translate_y_beg: -translate_y_end]\n",
    "    fliplr_img = np.fliplr(crop_img)\n",
    "    img_resize = cv2.resize(fliplr_img,(IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    return(img_resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Applying the augmentations #########################################################\n",
    "\n",
    "# Bounding boxes and resize\n",
    "def Bounding_Boxes(image, bounding_cord):\n",
    "    img = image[bounding_cord[1]:bounding_cord[3],bounding_cord[0]:bounding_cord[2],:]\n",
    "    img_resize = cv2.resize(img,(IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    return (img_resize)\n",
    "\n",
    "# Save the augmented pictures with right name and add them into the dataframe\n",
    "def Save_img(image_name,images_aug,j_aug,df,df2,idx):\n",
    "    j_filename, j_file_extension = os.path.splitext(image_name)\n",
    "    cv2.imwrite('{}/{}_aug_{}{}'.format(TRAIN_FOLDER_FINAL,j_filename,j_aug , j_file_extension), images_aug)\n",
    "    df2 = df2.append({'Image':'{}_aug_{}{}'.format(j_filename, j_aug, j_file_extension), 'Id':df['Id'].loc[idx]}, ignore_index=True)\n",
    "    return(df2)\n",
    "\n",
    "if START_FROM_SCRATCH:   \n",
    "    \n",
    "    # Loading the previous dataframe, delete the directory if already exists\n",
    "    if os.path.isdir(TRAIN_FOLDER_FINAL):\n",
    "        shutil.rmtree(TRAIN_FOLDER_FINAL)\n",
    "    os.makedirs(TRAIN_FOLDER_FINAL)\n",
    "    df        = pd.read_csv(TRAINING_FILE_3_NO_W) \n",
    "    df_t_bbox = pd.read_csv(BB_TRAIN).set_index('Name')\n",
    "    columns   = ['Image', 'Id']\n",
    "    df2       = pd.DataFrame(columns=columns)\n",
    "    df2.set_index('Image')\n",
    "\n",
    "    # for each picture add the original and it augmentation into the TRAIN_FOLDER_FINAL(the train folder for the model).\n",
    "    for idx, i in enumerate(df['Image']):\n",
    "      Augmentation_number = 1\n",
    "      image_path = '{}/{}'.format(TRAIN_FOLDER_3CHANNEL, i)\n",
    "      image = cv2.imread(image_path)\n",
    "\n",
    "      # If the image doesn't have bounding box just resize it.\n",
    "      try:\n",
    "          imagenp = Bounding_Boxes(np.array(image), df_t_bbox.loc[i])\n",
    "      except:\n",
    "          image   = cv2.imread(image_path)\n",
    "          imagenp = cv2.resize(image,(IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "            \n",
    "      # Save the mages in the final folder and add them the the dataframe.\n",
    "      cv2.imwrite('{}/{}'.format(TRAIN_FOLDER_FINAL, i), imagenp)\n",
    "      df2 = df2.append({'Image': '{}'.format(i), 'Id':df['Id'].loc[idx]}, ignore_index=True)\n",
    "           \n",
    "      images_aug = Augmentation_GaussianBlur().augment_images([imagenp])[0]\n",
    "      df2 = Save_img(i,images_aug,Augmentation_number,df,df2,idx)\n",
    "      Augmentation_number += 1\n",
    "       \n",
    "      images_aug = Augmentation_Scale_Trans(imagenp)\n",
    "      df2 = Save_img(i,images_aug,Augmentation_number,df,df2,idx)\n",
    "      Augmentation_number += 1\n",
    "\n",
    "      images_aug = Augmentation_Scale_Trans_Flip(imagenp)\n",
    "      df2 = Save_img(i,images_aug,Augmentation_number,df,df2,idx)\n",
    "      Augmentation_number += 1\n",
    "\n",
    "    df2.to_csv(TRAINING_FILE_FINAL_A)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a validation set\n",
    "Among each of the pictures belonging to an 'Id', one were choosen randomly to displaced into the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################ Create validation set #########################################################\n",
    "\n",
    "if START_FROM_SCRATCH:\n",
    "    \n",
    "    # Loading the previous dataframe, delete the directory if already exists\n",
    "    if os.path.isdir(VALID_FOLDER_FINAL):\n",
    "        shutil.rmtree(VALID_FOLDER_FINAL)\n",
    "    os.makedirs(VALID_FOLDER_FINAL)\n",
    "    df           = pd.read_csv(TRAINING_FILE_FINAL_A)\n",
    "    df_t         = df\n",
    "    whale_id     = df_t.groupby(\"Id\").Image.nunique()\n",
    "    whale_id_num = df_t.groupby(\"Id\").Image.nunique()[1]\n",
    "    columns      = ['Image', 'Id']\n",
    "    df2          = pd.DataFrame(columns=columns)\n",
    "    df2.set_index('Image')\n",
    "\n",
    "    number_of_valid    = len(whale_id.index)\n",
    "    whale_id_key       = whale_id.keys()\n",
    "    whale_id_val       = whale_id.values \n",
    "    whale_id_index     = 0\n",
    "    flip               = True\n",
    "    \n",
    "    # for each pictures belonging to an 'Id', one were choosen randomly to displaced into the validation set\n",
    "    # take VALIDIDATION_PRECENT percent to the validatoin set \n",
    "    while (number_of_valid != 0):\n",
    "           \n",
    "        Ident    = whale_id_key[whale_id_index] \n",
    "        cot_num  = whale_id_val[whale_id_index]\n",
    "        \n",
    "        if flip:\n",
    "            num_to_take = int(math.ceil(cot_num * VALIDIDATION_PRECENT))\n",
    "        else: \n",
    "            num_to_take = int(math.floor(cot_num * VALIDIDATION_PRECENT))\n",
    "        flip = not flip\n",
    " \n",
    "        for i in range(0,num_to_take): \n",
    "            choose       = random.randint(0,cot_num - 1)\n",
    "            image_picked = df_t.loc[df_t['Id'] == Ident].values[choose][1]  \n",
    "            df_t.drop(df_t.loc[df_t['Image'] == image_picked].index, inplace = True)  \n",
    "            cot_num     -= 1\n",
    "            df2          = df2.append({'Image':image_picked, 'Id':Ident}, ignore_index=True)\n",
    "            shutil.move(TRAIN_FOLDER_FINAL+'/'+image_picked, VALID_FOLDER_FINAL) \n",
    "            \n",
    "        number_of_valid -= 1\n",
    "        whale_id_index  += 1\n",
    "\n",
    "    df_t.to_csv(TRAINING_FILE_FINAL)  \n",
    "    df2.to_csv(VALID_FILE_FINAL)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------TRAINING--------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is based on the previous model that were learned in the course.\n",
    "The model used is InceptionResNetV2 without the Fully connected layers add average pooling with 2X2 filter and drop out of\n",
    "40%, follows by dense layer with softmax loss.\n",
    "Output best model within the range of \"EPOCH_WAIT_IMPROVE\" epochs.\n",
    "\n",
    "The model generated augmentations in-training-time and was optimized  with \"Adam\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################## Training #########################################################\n",
    "\n",
    "if DO_TRAINING:\n",
    "    df_t = pd.read_csv(TRAINING_FILE_FINAL)\n",
    "    df_t.drop(df_t.columns[0], axis=1,inplace=True)\n",
    "    df_v = pd.read_csv(VALID_FILE_FINAL)\n",
    "    df_v.drop(df_v.columns[0], axis=1,inplace=True)\n",
    "    \n",
    "    # data prep\n",
    "    train_datagen              = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input,\n",
    "        rotation_range         = 20,\n",
    "        width_shift_range      = 0.15,\n",
    "        height_shift_range     = 0.15,\n",
    "        shear_range            = 0.2,\n",
    "        zoom_range             = 0.2,\n",
    "        horizontal_flip        = False,\n",
    "        fill_mode              = 'nearest')\n",
    "    \n",
    "    validation_datagen         = ImageDataGenerator(\n",
    "        preprocessing_function = preprocess_input,\n",
    "        rotation_range         = 20,\n",
    "        width_shift_range      = 0.15,\n",
    "        height_shift_range     = 0.15,\n",
    "        shear_range            = 0.2,\n",
    "        zoom_range             = 0.2,\n",
    "        horizontal_flip        = False,\n",
    "        fill_mode              = 'nearest')\n",
    "    \n",
    "    train_generator            = train_datagen.flow_from_dataframe(\n",
    "        dataframe              = df_t,\n",
    "        directory              = TRAIN_FOLDER_FINAL,\n",
    "        x_col                  = 'Image',\n",
    "        y_col                  = 'Id',\n",
    "        batch_size             = BATCH_SIZE,\n",
    "        target_size            = TARGET_SIZES, \n",
    "        color_mode             = 'rgb')\n",
    "\n",
    "    validation_generator       = validation_datagen.flow_from_dataframe(\n",
    "        dataframe              = df_v,\n",
    "        directory              = VALID_FOLDER_FINAL,\n",
    "        x_col                  = 'Image',\n",
    "        y_col                  = 'Id',\n",
    "        batch_size             = BATCH_SIZE,\n",
    "        target_size            = TARGET_SIZES, \n",
    "        color_mode             = 'rgb')\n",
    "    \n",
    "    # setup model\n",
    "    base_model  = InceptionResNetV2(weights='imagenet', include_top=False)\n",
    "    x           = base_model.output\n",
    "    x           = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "    x           = Dropout(rate=0.4)(x)\n",
    "    predictions = Dense(CLASSES, activation='softmax')(x)\n",
    "    model       = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # count number of layers\n",
    "    count =0\n",
    "    for layer in base_model.layers:\n",
    "        count += 1\n",
    "\n",
    "    # transfer learning - 100% of the layers\n",
    "    iterator = iter(base_model.layers)\n",
    "    for num in range(round(count*1)):\n",
    "        next(iterator).trainable = False\n",
    "\n",
    "\n",
    "    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    model.compile(optimizer = adam,\n",
    "                  loss      = 'categorical_crossentropy',\n",
    "                  metrics   = ['accuracy'])\n",
    "    \n",
    "    # Number of files in folder\n",
    "    _, _, files         = next(os.walk(TRAIN_FOLDER_FINAL))\n",
    "    number_of_train_pic = len(files)\n",
    "    \n",
    "    _, _, files         = next(os.walk(VALID_FOLDER_FINAL))\n",
    "    number_of_valid_pic = len(files)\n",
    "    \n",
    "    tic = time.time() \n",
    " \n",
    "    early_stopping_callback = EarlyStopping(monitor='val_acc', patience=EPOCH_WAIT_IMPROVE)\n",
    "    \n",
    "    ###Unmarke it in inorder to save the best model in each epoch - very time consuming###\n",
    "    # checkpoint_callback     = ModelCheckpoint(MODEL_FILE, monitor='val_acc', verbose=1, /\n",
    "    #                                  save_weights_only = True, save_best_only=True, mode='max')\n",
    "\n",
    "    history = model.fit_generator(\n",
    "                train_generator,\n",
    "                epochs              = EPOCHS,\n",
    "                steps_per_epoch     = int(round(number_of_train_pic // BATCH_SIZE)),\n",
    "                validation_data     = validation_generator,\n",
    "                validation_steps    = int(round(number_of_valid_pic // BATCH_SIZE)),\n",
    "                callbacks           = [early_stopping_callback])\n",
    "  \n",
    "    toc = time.time()\n",
    "\n",
    "    print('Transfer learning time: %fs' % (toc - tic))\n",
    "    \n",
    "    # Save the history\n",
    "    with open(HISTORY_FILE, 'wb') as f: \n",
    "        pickle.dump(history, f)\n",
    "        \n",
    "    # Save the time for training\n",
    "    training_time_tictoc = toc - tic\n",
    "    \n",
    "    with open(MODEL_TIME, 'wb') as f: \n",
    "        pickle.dump(training_time_tictoc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the current model\n",
    "if DO_TRAINING:\n",
    "    model.save(MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer learning time: 5085.564309835434\n"
     ]
    }
   ],
   "source": [
    "####################################### Plotting the accuracy history #########################################################\n",
    "\n",
    "def plot_training(history):\n",
    "  acc = history.history['acc']\n",
    "  val_acc = history.history['val_acc']\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "  epochs = range(len(acc))\n",
    "  \n",
    "  plt.plot(epochs, acc, 'r.')\n",
    "  plt.plot(epochs, val_acc, 'r')\n",
    "  plt.title('Training and validation accuracy')\n",
    "  \n",
    "  plt.figure()\n",
    "  plt.plot(epochs, loss, 'r.')\n",
    "  plt.plot(epochs, val_loss, 'r-')\n",
    "  plt.title('Training and validation loss')\n",
    "  plt.show()\n",
    "\n",
    "if  DO_TRAINING:\n",
    "\n",
    "    # Loading the history\n",
    "    with open(HISTORY_FILE, 'rb') as f:  \n",
    "        history = pickle.load(f)\n",
    "        \n",
    "    # Loading the time for training\n",
    "    with open(MODEL_TIME, 'rb') as f: \n",
    "        training_time_tictoc = pickle.load(f)\n",
    "        \n",
    "    # plot the accuracy of the validation set and the training set\n",
    "    plot_training(history)\n",
    "   \n",
    "    # Tell how long ran the training\n",
    "    print('Transfer learning time: {}'.format(training_time_tictoc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous model\n",
    "if not DO_TRAINING:\n",
    "    model = load_model(MODEL_FILE, compile=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------PREDICTIONS----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction were made by test-time-augmentation(TTA).\n",
    "According to https://www.kaggle.com/andrewkh/test-time-augmentation-tta-worth-it it gives better results.\n",
    "Four versions of the predicted pictures were added:\n",
    "    1. The original picture.\n",
    "    2. The original picture fliped Left-Right.\n",
    "    3. Translation of the picture in random amount. Left-Up(0-20), Right-Down(0-20) and rescale it.\n",
    "    4. Translation of the picture like before and add flip left-right and bluring.\n",
    "\n",
    "A threshold for adding 'new_whale' Id, to the predictions that don't have a clear identification.\n",
    "It was added in the place(out of 5) that first didn't met the threshold.\n",
    "At the end a submission file were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################### Predictions #########################################################\n",
    "from keras.preprocessing   import image\n",
    "# Function for finding the most common prediction number.\n",
    "def Most_common_count(list_of_arg):\n",
    "    return(Counter(list_of_arg).most_common(1)[0][1])\n",
    "\n",
    "# Function for finding the most common prediction element.\n",
    "def Most_common_element(list_of_arg):\n",
    "    return(Counter(list_of_arg).most_common(1)[0][0])\n",
    "\n",
    "# Function for finding the index.\n",
    "def Index_of_value(list_of_arg, num_to_index):\n",
    "    return(list_of_arg.index(num_to_index))\n",
    "\n",
    "# Bounding boxes and resize\n",
    "def Bounding_Boxes(image, bounding_cord):\n",
    "    img = image[bounding_cord[1]:bounding_cord[3],bounding_cord[0]:bounding_cord[2],:]\n",
    "    img_resize = cv2.resize(img,(IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    return (img_resize)\n",
    "\n",
    "# Translation of the picture in random amount. Left-Up(0-20), Right-Down(0-20) and rescale it\n",
    "def Augmentation_Scale_Trans(imagenp):\n",
    "    translate_x_beg = random.randint(1,21)\n",
    "    translate_y_beg = random.randint(1,21)\n",
    "    translate_x_end = random.randint(1,21)\n",
    "    translate_y_end = random.randint(1,21)\n",
    "    crop_img = imagenp[translate_x_beg: -translate_x_end, translate_y_beg: -translate_y_end]\n",
    "    img_resize = cv2.resize(crop_img,(IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    return(img_resize)\n",
    "\n",
    "# Translation of the picture like before and add flip left-right and bluring.\n",
    "def Augmentation_Scale_Trans_Flip_Blur(imagenp):\n",
    "    translate_x_beg = random.randint(1,21)\n",
    "    translate_y_beg = random.randint(1,21)\n",
    "    translate_x_end = random.randint(1,21)\n",
    "    translate_y_end = random.randint(1,21)\n",
    "    crop_img = imagenp[translate_x_beg: -translate_x_end, translate_y_beg: -translate_y_end]\n",
    "    fliplr_img = np.fliplr(crop_img)\n",
    "    blur_img = cv2.filter2D(fliplr_img,-1,np.ones((5,5),np.float32)/25)  # Filter ot 5X5\n",
    "    img_resize = cv2.resize(blur_img,(IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "    return(img_resize)\n",
    "\n",
    "# Function that does TTA\n",
    "def my_TTA(img):\n",
    "\n",
    "  # Set the indication if new whale predicted already for this picture to false.\n",
    "  # Set the place to put the new_whale predication to the end of the list(ignored there).\n",
    "  # Preprocess the image before insert it into the predic model.\n",
    "  Is_new_whale       = False\n",
    "  new_whale_location = NUMBER_OF_PREDICATIONS\n",
    "  x                  = image.img_to_array(img)\n",
    "  \n",
    "  # Augmentation for the picture as described.\n",
    "  original_img       = np.array(x) \n",
    "\n",
    "  original_img       = np.expand_dims(original_img, axis=0)\n",
    "  original_img       = preprocess_input(original_img)\n",
    "\n",
    "  predictions_orig   = model.predict(original_img, verbose=0)    \n",
    "  flip_img           = np.array(Augmentation_Scale_Trans_Flip_Blur(np.array(x)))\n",
    "\n",
    "  flip_img           = np.expand_dims(flip_img, axis=0)\n",
    "  flip_img           = preprocess_input(flip_img)\n",
    "\n",
    "  predictions_flip   = model.predict(flip_img, verbose=0)\n",
    "  fliplr_img         = np.fliplr(np.array(x))\n",
    "\n",
    "  fliplr_img         = np.expand_dims(fliplr_img, axis=0)\n",
    "  fliplr_img         = preprocess_input(fliplr_img)\n",
    "\n",
    "  predictions_fliplr = model.predict(fliplr_img, verbose=0)\n",
    "  flipud_img         = np.array(Augmentation_Scale_Trans_Flip_Blur(np.array(x)))\n",
    "\n",
    "  flipud_img         = np.expand_dims(flipud_img, axis=0)\n",
    "  flipud_img         = preprocess_input(flipud_img)\n",
    "\n",
    "  predictions_flipud = model.predict(flipud_img, verbose=0)\n",
    "  \n",
    "  # The first 5 predictions for each augmentation\n",
    "  predictions_orig_ind   = predictions_orig.argsort()[0][::-1][:NUMBER_OF_PREDICATIONS]\n",
    "  predictions_flip_ind   = predictions_flip.argsort()[0][::-1][:NUMBER_OF_PREDICATIONS]\n",
    "  predictions_fliplr_ind = predictions_fliplr.argsort()[0][::-1][:NUMBER_OF_PREDICATIONS]\n",
    "  predictions_flipud_ind = predictions_flipud.argsort()[0][::-1][:NUMBER_OF_PREDICATIONS]  \n",
    "  llist           = []\n",
    "  Best_guess_list = []\n",
    "  Index_list      = [0,0,0,0]\n",
    "\n",
    "  # For each picture find the correct prediction according the described logic.\n",
    "  for i in range(NUMBER_OF_PREDICATIONS):\n",
    "    llist        = [predictions_orig_ind[Index_list[0]], predictions_flip_ind[Index_list[1]], \\\n",
    "                     predictions_fliplr_ind[Index_list[2]], predictions_flipud_ind[Index_list[3]]]\n",
    "    max_pred_num = [predictions_orig[0][Index_list[0]], predictions_flip[0][Index_list[1]], \\\n",
    "                        predictions_fliplr[0][Index_list[2]], predictions_flipud[0][Index_list[3]]]\n",
    " \n",
    "    max_pred_value = max_pred_num[max_pred_num.index(max(max_pred_num))]\n",
    "\n",
    "    # If the prediction is not above the threshold than put a new_whale classification in that spot\n",
    "    # and ignore futher classifiaction below the thresholf for that picture.\n",
    "    if not Is_new_whale and max_pred_value < THRESHOLD:\n",
    "        Is_new_whale       = True\n",
    "        new_whale_location = i\n",
    "    \n",
    "    # If there are more than one prediction agreement on the classification, choose this classification,\n",
    "    # otherwise choose the highest prediction.\n",
    "    if Most_common_count(llist) > 1:\n",
    "        Best_guess_list.append(Most_common_element(llist))\n",
    "        Index_list[Index_of_value(llist, Most_common_element(llist))] += 1\n",
    "    else:\n",
    "        Best_guess_list.append(max_pred_num.index(max_pred_value))\n",
    "        Index_list[max_pred_num.index(max_pred_value)] += 1\n",
    "\n",
    "  # Insert the 'new_whale' classification into the predictions.\n",
    "  if Is_new_whale:\n",
    "    Best_guess_list.insert(new_whale_location, NEW_WHALE_INDEX)\n",
    "    del Best_guess_list[-1]\n",
    "\n",
    "  return(Best_guess_list)       \n",
    "\n",
    "# Turn the Id's into numbers-labels for decode them.\n",
    "train_df        = pd.read_csv(TRAINING_FILE)\n",
    "values          = np.array(train_df['Id'])\n",
    "label_encoder   = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(values)\n",
    "\n",
    "# Load the previous created subbmision file and the test bounding boxes\n",
    "test_df   = pd.read_csv(CONV_SUB_FILE).set_index('Image')\n",
    "df_s_bbox = pd.read_csv(BB_TEST).set_index('Name')\n",
    "\n",
    "# for each picture in the test do TTA and if it dosen't aleady exists in the submissin file, add it predication.\n",
    "for fig in tqdm(test_df.index):\n",
    "  image_path = '{}/{}'.format(TEST_FOLDER, fig)\n",
    "  img = cv2.imread(image_path)\n",
    "\n",
    "  # If the image doesn't have bounding box, just resize it.\n",
    "  try:\n",
    "      img = Bounding_Boxes(np.array(img), df_s_bbox.loc[fig])\n",
    "  except:\n",
    "      img     = cv2.imread(image_path)\n",
    "      img     = cv2.resize(img,(IMAGE_SIZE, IMAGE_SIZE), interpolation = cv2.INTER_CUBIC)\n",
    "  if test_df.loc[fig,'Id'] == ' ':\n",
    "      test_df.loc[fig, 'Id'] = \" \".join(label_encoder.inverse_transform(my_TTA(img)))\n",
    "\n",
    "# Save submission file\n",
    "test_df.to_csv(SUBMISSION_FILE, index=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
